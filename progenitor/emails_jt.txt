Hi John,

How are you? I’m currently working on a native grid data assimilation system and we’re looking to
compute stream function and velocity potential for a number of grids, in particular FV3, LFRic and
MPAS without a remapping to lat-lon. The objective being to model background error covariances
directly on these grids. Given your PV work on icosahedral grids I wondered if you have any solvers
for computing stream function and velocity potential that you might be willing to share, or if you
can point me to some resources that might be useful? I’m keen not to reinvent the wheel too much if
possible and know there’s potentially a lot out there on these elliptic solvers in the dynamical
core community, although not sure what’s out there for non-lat-lon grids.

Thanks,
Dan.

!---------------------------------------------------------------------------------------------------

Hi Dan,

Yes, indeed. As part of the Gung Ho project I developed two shallow water models,
one using finite volumes, one using finite elements, and both worked on either
a cubed sphere grid or an icosahedral grid. And both included Helmholtz solvers
for semi-implicit time stepping, which could be adapted into Poisson solvers.

There are sure to be some restrictions/limitations on whether they will work.
I will try to remind myself of the details and hope to get back to you later today.

Cheers,
John

!---------------------------------------------------------------------------------------------------

Hi Dan,

Having thought about it a bit more, I think the finite element method
(Thuburn and Cotter 2015, J. Comp. Phys.) is
probably better. (The FV method requires certain restrictions on the grid,
and is less accurate anyway.) I am assuming there is no requirement for
exact numerical consistency between the models' own discretization and that
used to solve the Poisson problem. It shouldn't make any difference, except near the
grid scale where we don't trust the details anyway.

The solver uses a multigrid method. Therefore, there needs to exist a hierarchy of
grids with the desired resolution being the finest in the hierarchy. That should not be a
problem for the quasi-uniform versions of either the MPAS or FV3 grids, since both can
be generated by iterative bisection of some very coarse grid. But it may limit the choices
of resolution. It might also make things tricky on stretched versions of the grids.
(In principle there doesn't need to be a nice 2:1 ratio between grids in the hierarchy,
but then much more work is needed to figure out and implement the coarse <-> fine
grid transfer operators.)

My code isn't parallelized, but since each layer is independent of every other one
we could easily parallelize over layers. With a bit more work one could do a
horizontal domain decomposition as we did for our implicit version of MPAS
(Sandbach et al. Mon. Wea. Rev. 2015).

If you think this sounds reasonable then I will try to extract the solver bit from
my code and set it up as a stand-alone solver.

Cheers,
John

!---------------------------------------------------------------------------------------------------

Hi John

Thanks so much for spending time on this, very much appreciated.

I don’t see any issue with using finite element instead of finite volume to do the transform within
the data assimilation algorithm. I think it should be fairly straightforward to handle the different
grid geometries for FV3, I’ll take a read of the paper you mentioned and start preparing some grids.
We have no intention of running any kind of stretched grid data assimilation at the moment so that
shouldn’t be an issue either.

I’d be happy to work on the horizontal domain decomposition. Hopefully we can use some of the domain
decomposition methods packaged with the dynamical cores but I’d need to take a look at the details.
Presumably some halo points for the horizontal discretization and then some kind of global
communication to check for convergence?

I hope it’s not too much work but if possible to extract the solver fairly easily that would be
brilliant!

Many thanks,
Dan.

!---------------------------------------------------------------------------------------------------

Hi Dan,

Sorry it took a bit longer than expected, but here is some code, and a long
(but far from complete) email of explanation.  Having thought more about it,
the hard part is going to be figuring out how to take the grid information
you have for your models and combine it with my code to generate all the
info the solver needs.  I'm very happy to discuss further.  But for now try out the
code and take a look to get a feeling for whether it is something that you might be
able to use.

Cheers,
John


The tar file should contain the following files:
gengrid_hex.f           Generate a hexagonal-icosahedral grid
dodecahedron.xref       Input file for gengrid_hex.f
gengrid_cube2.f90       Generate a cubed sphere grid
buildop_fem2.f90        Build mass matrices, intergrid transfer operators, etc
fempoisson.f90          Code to solve a Poisson problem on a given input grid
poissonnml.in           Namelist file for fempoisson.f90


Compiling

The codes are plain fortran77 or fortran90. I don't use any fancy compiler
options except some optimization to speed things up. E.g.
gfortran -O3 fempoisson.f90


Building grids

To build a hexagonal grid compile and run gengrid_hex.f
When the program asks `Create a GRIDMAP file (0 or 1) ?' enter 1
It should create a file with a name like gridmap_hexa_0000040962.dat
(the number is the number of faces on the grid.
To change the resolution you need to do a GLOBAL change on the string
NGRIDS=7 (it occurs in various places in the code).
Note the code is slow because it iteratively (and very inefficiently)
tweaks the grid to either do Heikes-Randall optimization or to make
it centroidal (like MPAS).

To build a cubed sphere grid compile and run gengrid_cube2.f90.
It should create a file with a name like gridmap_eqcu_0000055296.dat
To change the resolution change the parameter ngrids (and possibly n0)
in module grid.

See below for more details on the grids...


Building operators

Unlike some finite element codes which do these things on the fly, my code
precomputes mass matrices and other operators it needs such as restriction and
prolongation operators for the multigrid solver.
To do this, compile and run buildop_fem2.f90
The code will ask for information about the grid - this is simply to allow it to
build the name of the grid file that it will read in. The code then builds
the various operators and then writes both the grid information and the operator
information to a file with a name like gridopermap_hexa_0000040962.dat
(The code actually generates more operators than we actually need for the
Poisson problem, but I didn't bother to strip out the unnecessary ones.)


Solving a sample Poisson problem.

The code fempoisson.f90 solves a sample poisson problem. It will use the grid
that is specified in the namelist file poissonnml.in. After some preliminary
stuff to set things up, it defines a stream function, calculates the Laplacian
to get the vorticity, then solves the Poisson problem to (hopefully) get back the
original stream function. It prints out some diagnostics to check that the
solution is indeed converging to the correct answer. More details below.


Grids

The biggest thing you'll need to think about is the grids.

The multigrid solver needs a nested hierarchy of grids. It is much simpler to
generate these all together, rather than try and back out the coarser grids from
a given fine grid.

Quite a lot of information about the grid is used - see the description in module grid
(which should be the same or very similar in buildop_fem2.f90 and fempoisson.f90).
You will need to figure out how to get this information for the models you're using.
Actually, MPAS uses very similar grid connectivity information, so you may be able to
exploit that. You may be able to take the grid coordinates and some basic connectivity
information and then use bits of my code to construct the rest from what you've got.


The solver

The Poisson problem is discretized in the form
D_2 M^{-1} \bar{D}_1 L psi  =  zeta

psi and zeta are the stream function and vorticity weighted by the area of grid cells.
D_2 and \bar{D}_1 are incidence matrices. Their entries are +1, -1 and (mostly) 0.
They describe certain aspects of the grid connectivity (which edges border which cells,
etc) and the contain the essence of the div and grad operators.
L is a mass matrix, but it is diagonal, so easy to deal with.
M is another mass matrix. Although it is sparse, its inverse is not. This makes the problem
trickier to solver.

The multigrid solver solves an approximate problem
D_2 \hat{M}^{-1} \bar{D}_1 L psi  =  zeta
where \hat{M}^{-1} is a sparse approximation to M^{-1}

Because this is not solving exactly the right problem, it is embedded within an outer loop
that iteratively improves the approximation to M^{-1} \bar{D}_1 L psi,
and then uses the multigrid solver to compute a correction to psi.

In the tests I did, the error in psi reduced by about 50% at each iteration of the
outer loop. This is quite slow but will get there eventually.

There are options for tuning various parameters, which might improve the convergence,
such as the parameter relax in buildxminv, or including an under/over relaxation parameter
in the outer solver loop.


You're sure to have lots of questions, so I'm very happy to discuss further.

!---------------------------------------------------------------------------------------------------\

Hi John,

Thanks again for sharing your code and for the detailed instructions. I’ve been having a play around
with the solver and had a quick question before I go further on trying to sync with the FV3 grid
structure and refactor the top level to hook into our DA code. We will ultimately need to have the
adjoint of the solver for it to be useful. I can work on that but from your knowledge of the
structure are there any alarm bells that immediately ring on trying to do that?

Thanks,
Dan.

!---------------------------------------------------------------------------------------------------

Hi Dan,

Good news: The discrete Laplacian operator (at least for this finite element method) is
[essentially] self adjoint. So you may not need to code the adjoint of the method
at all. (I say `essentially' because there are some area factors that need to be thought
about, but they should be trivial once understood.)

One other thought occurred to me. Instead of the outer fixed point iteration loop
in the code I sent, it might be better to use some find of Krylov subspace method
like conjugate gradient (then the multigrid would be used as a preconditioner).
So I think the solver could almost certainly be made to converge faster.

Cheers,
John

!---------------------------------------------------------------------------------------------------

Hi John,

Apologies for the gap in communication, just getting back to the Poisson solver work. Given the
slight differences in grid spacing my initial implementation of your solver was to simply
interpolate my fields to your cubed sphere grid and use the solver as is. Now I would like to try
and build the solver for the actual FV3 grid. But before I start I just wanted to check with you
that if I can essentially provide the fields output by gengrid_cube2.f90 (listed below) I can then
use buildop_fem2.f90 and the rest of the solver without any problems? Will test things carefully
but in case you know of anything that expects something specific about the grid spacing. I presume
not since the same code is used for cube and hex but just wanted to check before I go further,
perhaps the “multigrid restriction operator” is a problem?

The second thing that I’m unclear on relates to the number of edges. I think this may be a
difference between FV3 and your cubed sphere grid but seems you have two edges between faces,
a ‘right’ edge and a ‘left’ edge for example. Is that indeed the case?

Thanks,
Dan.


Fields to be provided:
fnxtf(:,:,:), eoff(:,:,:), voff(:,:,:), &
                        fnxte(:,:,:), vofe(:,:,:), &
                        fofv(:,:,:), eofv(:,:,:)
eoffin(:,:,:), eofvin(:,:,:)
flong(:,:), flat(:,:), &
                       vlong(:,:), vlat(:,:), &
                       farea(:,:), varea(:,:), &
                       ldist(:,:), ddist(:,:), &
                       fareamin(:)

!---------------------------------------------------------------------------------------------------

Hi Dan,

Yes, if you can provide those fields then you should be able to plug them straight into
builop_fem2.f90.

You are right to spot the restriction operator as potentially tricky, since the buildop code
assumes a certain grid cell numbering convention. So you may need to rewrite the subroutine
buildinj for your purposes. On the cubed sphere the natural restriction
operator would take a 2x2 block of cells on a fine grid and map those to one cell on the next
coarser grid.  So as long as you can figure out the fine cell indices (rhs of lines 3-6 below) that
go with a given coarse cell index (if0) you should be fine. There are always four cells in the
stencil (line 2) and the weights are always 1.0 (line 7).  The prolongation operator is
(essentially) the adjoint of the restriction operator, so there's no need to code that separately.

         if0 = t1 + (p - 1)*n2
          ninj(if0,igrid) = 4
          injsten(if0,1,igrid) = s1 + (p - 1)*n2p
          injsten(if0,2,igrid) = s2 + (p - 1)*n2p
          injsten(if0,3,igrid) = s3 + (p - 1)*n2p
          injsten(if0,4,igrid) = s4 + (p - 1)*n2p
          injwgt(if0,1:4,igrid) = 1.0d0

There is only one edge between any pair of faces, no `left' and `right'  (though we do have to keep
track of
which is the `positive' direction). Is there a specific piece of code that does not make sense?

'Hope that helps, but don't hesitate to get back with any questions.

Cheers,
John

!---------------------------------------------------------------------------------------------------
